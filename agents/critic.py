'''
This module defines the CriticAgent, responsible for evaluating candidate
outputs generated by the SmallScientificLLM. 
'''

import re

import torch
from torch.nn import Embedding
import numpy as np

from models.slm import SmallScientificLLM
from tokenizer import tokenizer
from config import DEVICE, DOMAINS, MAX_SEQ_LEN, HIDDEN_SIZE


class CriticAgent:
    """
    CriticAgent evaluates candidate outputs for scientific correctness
    using a combination of symbolic heuristics, neural embeddings, and 
    optimized scoring.

    Attributes:
        embedding_layer (torch.nn.Embedding): Embedding layer for tokenized input.
        model (SmallScientificLLM): Pretrained small scientific LLM for evaluation.
    """

    def __init__(self):
        """
        Initializes the CriticAgent with a neural embedding layer and
        a SmallScientificLLM model in evaluation mode.
        """
        self.embedding_layer = Embedding(tokenizer.vocab_size, HIDDEN_SIZE).to(DEVICE)
        self.model = SmallScientificLLM().to(DEVICE)
        self.model.eval()  # Set to evaluation mode to avoid gradients

    def evaluate(self, candidates):
        """
        Quickly scores multiple candidates based on overall evaluation metrics.

        Args:
            candidates (List[str]): List of candidate strings to evaluate.

        Returns:
            List[float]: List of overall_scores (0-1) for each candidate.
        """
        scores = []
        for candidate in candidates:
            metrics = self.evaluate_metrics(candidate)
            scores.append(metrics["overall_score"])
        return scores

    def evaluate_metrics(self, candidate: str):
        """
        Performs a detailed evaluation of a single candidate output.

        The evaluation combines:
            1. Symbolic/heuristic checks for contradictions and hallucinations.
            2. Neural semantic consistency using SmallScientificLLM.
            3. Optimized scoring with weighted combination of metrics.

        Args:
            candidate (str): Candidate output text to evaluate.

        Returns:
            dict: Dictionary containing the following keys:
                - contradictions (float): Normalized score (0-1) of detected contradictions.
                - hallucinations (float): Normalized score (0-1) of detected hallucinations.
                - reasoning_quality (float): Semantic consistency score (0-1).
                - overall_score (float): Weighted overall evaluation score (0-1).
        """
        # Step 1: Symbolic / heuristic checks
        contradictions = 0.0
        hallucinations = 0.0

        if re.search(r"\bnot\b|\bnever\b|\bimpossible\b", candidate):
            contradictions += 0.2

        if re.search(r"[A-Z]{3,}", candidate):
            hallucinations += 0.1

        # Step 2: Neural semantic evaluation
        tokens = tokenizer.encode(candidate, return_tensors="pt").to(DEVICE)
        if tokens.size(1) > MAX_SEQ_LEN:
            tokens = tokens[:, :MAX_SEQ_LEN]
        elif tokens.size(1) < MAX_SEQ_LEN:
            pad_len = MAX_SEQ_LEN - tokens.size(1)
            tokens = torch.cat(
                [tokens, torch.zeros(1, pad_len, dtype=torch.long).to(DEVICE)], dim=1
            )
        input_tensor = self.embedding_layer(tokens)
        x_dict = {domain: input_tensor for domain in DOMAINS}

        with torch.no_grad():
            model_out = self.model(x_dict, chat_tensor=None)
            # Measure self-consistency via variance of domain outputs
            domain_variances = torch.var(model_out, dim=1).mean().item()
            reasoning_quality = 1.0 / (1.0 + domain_variances)  # Higher variance -> lower quality

        # Step 3: Optimization aggregation
        overall_score = max(
            0.0,
            min(
                1.0,
                reasoning_quality * 0.7 + (1 - contradictions) * 0.2 + (1 - hallucinations) * 0.1
            ),
        )

        return {
            "contradictions": contradictions,
            "hallucinations": hallucinations,
            "reasoning_quality": reasoning_quality,
            "overall_score": overall_score,
        }

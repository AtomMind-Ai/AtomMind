"""
This module defines the CriticAgent, responsible for evaluating candidate
outputs generated by the SmallScientificLLM.
"""

import re

import torch
from torch.nn import Embedding

from models.slm import SmallScientificLLM
from tokenizer import tokenizer
from config import DEVICE, DOMAINS, MAX_SEQ_LEN, HIDDEN_SIZE
from utils.tokens import adjust_seq_len

class CriticAgent:
    """
    CriticAgent evaluates candidate outputs for scientific correctness
    using a combination of symbolic heuristics, neural embeddings, and
    optimized scoring.
    """

    def __init__(self):
        """
        Initializes the CriticAgent with a neural embedding layer and
        a SmallScientificLLM model in evaluation mode.
        """
        self.embedding_layer = Embedding(tokenizer.vocab_size, HIDDEN_SIZE).to(DEVICE)
        self.model = SmallScientificLLM().to(DEVICE)
        self.model.eval()  # Evaluation mode to avoid gradients

    def evaluate(self, candidates):
        """
        Quickly scores multiple candidates based on overall evaluation metrics.

        Args:
            candidates (List[str]): List of candidate strings to evaluate.

        Returns:
            List[float]: List of overall_scores (0-1) for each candidate.
        """
        return [self.evaluate_metrics(candidate)["overall_score"] for candidate in candidates]

    def evaluate_metrics(self, candidate: str):
        """
        Performs a detailed evaluation of a single candidate output.

        Combines symbolic heuristics and neural evaluation to produce
        a weighted overall score.
        """
        # Step 1: Symbolic / heuristic checks
        contradictions = 0.0
        hallucinations = 0.0

        if re.search(r"\bnot\b|\bnever\b|\bimpossible\b", candidate):
            contradictions += 0.2

        if re.search(r"[A-Z]{3,}", candidate):
            hallucinations += 0.1

        # Step 2: Neural semantic evaluation
        tokens = tokenizer.encode(candidate, return_tensors="pt").to(DEVICE)
        tokens = adjust_seq_len(tokens, MAX_SEQ_LEN)  # Use helper function

        input_tensor = self.embedding_layer(tokens)
        x_dict = {domain: input_tensor for domain in DOMAINS}

        with torch.no_grad():
            model_out = self.model(x_dict, chat_tensor=None)
            domain_variances = torch.var(model_out, dim=1).mean().item()
            reasoning_quality = 1.0 / (1.0 + domain_variances)  # Higher variance -> lower quality

        # Step 3: Optimization aggregation
        overall_score = max(
            0.0,
            min(
                1.0,
                reasoning_quality * 0.7 + (1 - contradictions) * 0.2 + (1 - hallucinations) * 0.1,
            ),
        )

        return {
            "contradictions": contradictions,
            "hallucinations": hallucinations,
            "reasoning_quality": reasoning_quality,
            "overall_score": overall_score,
        }
